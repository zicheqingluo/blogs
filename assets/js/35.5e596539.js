(window.webpackJsonp=window.webpackJsonp||[]).push([[35],{487:function(t,e,a){"use strict";a.r(e);var r=a(1),s=Object(r.a)({},(function(){var t=this,e=t._self._c;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("h2",{attrs:{id:"环境配置"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#环境配置"}},[t._v("#")]),t._v(" 环境配置")]),t._v(" "),e("h3",{attrs:{id:"环境要求"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#环境要求"}},[t._v("#")]),t._v(" 环境要求")]),t._v(" "),e("p",[t._v("win10")]),t._v(" "),e("p",[t._v("pyenv")]),t._v(" "),e("h3",{attrs:{id:"python环境"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#python环境"}},[t._v("#")]),t._v(" python环境")]),t._v(" "),e("p",[t._v("参考pyenv安装。")]),t._v(" "),e("h3",{attrs:{id:"安装cuda相关"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#安装cuda相关"}},[t._v("#")]),t._v(" 安装CUDA相关")]),t._v(" "),e("p",[t._v("参考英伟达驱动安装")]),t._v(" "),e("h3",{attrs:{id:"安装pytorch"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#安装pytorch"}},[t._v("#")]),t._v(" 安装pytorch")]),t._v(" "),e("p",[t._v("参考pytorch安装")]),t._v(" "),e("h3",{attrs:{id:"安装git-lfs"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#安装git-lfs"}},[t._v("#")]),t._v(" 安装git-lfs")]),t._v(" "),e("p",[t._v("从 Hugging Face Hub 下载模型需要先"),e("a",{attrs:{href:"https://docs.github.com/zh/repositories/working-with-files/managing-large-files/installing-git-large-file-storage",target:"_blank",rel:"nofollow noopener noreferrer"}},[t._v("安装Git LFS"),e("OutboundLink")],1),t._v("，然后运行")]),t._v(" "),e("p",[e("a",{attrs:{href:"https://git-lfs.com/",target:"_blank",rel:"nofollow noopener noreferrer"}},[t._v("点击安装"),e("OutboundLink")],1)]),t._v(" "),e("h2",{attrs:{id:"模型推理"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#模型推理"}},[t._v("#")]),t._v(" 模型推理")]),t._v(" "),e("p",[t._v("使用chatglm的过程属于推理过程。")]),t._v(" "),e("h3",{attrs:{id:"配置"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#配置"}},[t._v("#")]),t._v(" 配置")]),t._v(" "),e("p",[t._v("本次运行环境为win10(正常windows非WSL子系统)")]),t._v(" "),e("h4",{attrs:{id:"前置操作"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#前置操作"}},[t._v("#")]),t._v(" 前置操作")]),t._v(" "),e("p",[t._v("防止vpn导致无法访问")]),t._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v('# demo.queue().launch(share=False, inbrowser=True)改为\n\nos.environ["no_proxy"] = "localhost,127.0.0.1,::1"\ndemo.queue().launch(server_name="0.0.0.0", server_port=7890,share=False, inbrowser=True)\n')])]),t._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[t._v("1")]),e("br"),e("span",{staticClass:"line-number"},[t._v("2")]),e("br"),e("span",{staticClass:"line-number"},[t._v("3")]),e("br"),e("span",{staticClass:"line-number"},[t._v("4")]),e("br")])]),e("h4",{attrs:{id:"chatglm-6b"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#chatglm-6b"}},[t._v("#")]),t._v(" chatGLM-6B")]),t._v(" "),e("p",[t._v("通过chatGLM来在消费级显卡上运行大模型，项目已经准备好demo脚本可直接启动。")]),t._v(" "),e("p",[t._v("可先查看量化等级对硬件配置要求"),e("a",{attrs:{href:"https://github.com/THUDM/ChatGLM-6B",target:"_blank",rel:"nofollow noopener noreferrer"}},[t._v("chatGLM-6B"),e("OutboundLink")],1)]),t._v(" "),e("p",[e("code",[t._v("git clone https://github.com/THUDM/ChatGLM-6B.git")])]),t._v(" "),e("h4",{attrs:{id:"模型"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#模型"}},[t._v("#")]),t._v(" 模型")]),t._v(" "),e("p",[t._v("chatGLM-6B启动demo时会自动下载模型，但从transformers下载很慢，可以改为手动从国内镜像源下载，本地加载模型的方式实现。")]),t._v(" "),e("p",[t._v("因为国内镜像源也是通过git方式下载，所以对于模型参数来说下载可能还达不到满速，此时可以把模型分为模型实现和参数，模型实现可通过git下载(需安装git-lfs)，模型参数可以通过清华网站下载通过多线程可以到40Mbps")]),t._v(" "),e("ul",[e("li",[e("p",[t._v("下载模型实现")]),t._v(" "),e("p",[e("code",[t._v("GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/THUDM/chatglm-6b")])])]),t._v(" "),e("li",[e("p",[t._v("下载模型参数")]),t._v(" "),e("p",[e("a",{attrs:{href:"https://cloud.tsinghua.edu.cn/d/fb9f16d6dc8f482596c2/",target:"_blank",rel:"nofollow noopener noreferrer"}},[t._v("点击下载"),e("OutboundLink")],1),t._v(" ，参数需要放在上面git clone模型实现的目录下。")])])]),t._v(" "),e("p",[t._v("int4"),e("strong",[t._v("量化")])]),t._v(" "),e("p",[t._v("以上是标准模型，如果电脑配置低可使用量化后的模型")]),t._v(" "),e("ul",[e("li",[e("p",[t._v("模型实现 :"),e("code",[t._v("GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/THUDM/chatglm-6b-int4/tree/main")])])]),t._v(" "),e("li",[e("p",[e("a",{attrs:{href:"https://cloud.tsinghua.edu.cn/d/674208019e314311ab5c/?p=%2Fchatglm-6b-int4&mode=list",target:"_blank",rel:"nofollow noopener noreferrer"}},[t._v("int4模型参数"),e("OutboundLink")],1),t._v(" ，全部放到模型实现目录下")])])]),t._v(" "),e("h3",{attrs:{id:"运行"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#运行"}},[t._v("#")]),t._v(" 运行")]),t._v(" "),e("h4",{attrs:{id:"普通运行"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#普通运行"}},[t._v("#")]),t._v(" 普通运行")]),t._v(" "),e("p",[t._v("找到chatglm-6b下的"),e("code",[t._v("python web_demo.py")]),t._v(" ，把模型路径改成实际路径,windows下对\\转义")]),t._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("chatglm6b='D:\\tom\\\\ai\\huggingface.co\\\\chatglm-6b'\ntokenizer = AutoTokenizer.from_pretrained(chatglm6b, trust_remote_code=True)\nmodel = AutoModel.from_pretrained(chatglm6b, trust_remote_code=True).half().cuda()\n")])]),t._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[t._v("1")]),e("br"),e("span",{staticClass:"line-number"},[t._v("2")]),e("br"),e("span",{staticClass:"line-number"},[t._v("3")]),e("br")])]),e("h4",{attrs:{id:"int4量化"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#int4量化"}},[t._v("#")]),t._v(" int4量化")]),t._v(" "),e("p",[t._v("修改web_demo.py，增加quantize(4)参数，如果使用int8量化改为quantize(8)，前提是需要下载int量化的模型参考【配置】")]),t._v(" "),e("div",{staticClass:"language-python line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("chatglm6b"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'D:\\yxk\\\\ai\\huggingface.co\\\\chatglm-6b-int4'")]),t._v("\nmodel "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" AutoModel"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_pretrained"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("chatglm6b"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" trust_remote_code"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("quantize"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("half"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cuda"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[t._v("1")]),e("br"),e("span",{staticClass:"line-number"},[t._v("2")]),e("br")])]),e("h4",{attrs:{id:"使用cpu运行"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#使用cpu运行"}},[t._v("#")]),t._v(" 使用cpu运行")]),t._v(" "),e("p",[t._v("如果没有显卡可以通过cpu运行")]),t._v(" "),e("p",[t._v("改为：")]),t._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("model = AutoModel.from_pretrained(chatglm6b, trust_remote_code=True).float()\n")])]),t._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[t._v("1")]),e("br")])]),e("h4",{attrs:{id:"命令行运行"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#命令行运行"}},[t._v("#")]),t._v(" 命令行运行")]),t._v(" "),e("p",[t._v("需要安装readline，python3安装命令")]),t._v(" "),e("div",{staticClass:"language-python line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("pip install pyreadline3 "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("or")]),t._v(" python "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("m pip install pyreadline\n")])]),t._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[t._v("1")]),e("br")])]),e("h2",{attrs:{id:"报错"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#报错"}},[t._v("#")]),t._v(" 报错")]),t._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("AssertionError: Torch not compiled with CUDA enabled\n")])]),t._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[t._v("1")]),e("br")])]),e("ul",[e("li",[e("p",[t._v("原因")]),t._v(" "),e("p",[t._v("驱动为正确安装或不支持CUDA或使用运行方式不对")])]),t._v(" "),e("li",[e("p",[t._v("解法")]),t._v(" "),e("p",[t._v("如果使用cpu运行应该配置"),e("code",[t._v("model = AutoModel.from_pretrained(chatglm6b, trust_remote_code=True).float()")])]),t._v(" "),e("p",[t._v("如果使用gpu运行应该正确安装CUDA")])]),t._v(" "),e("li",[e("p",[t._v("参考")]),t._v(" "),e("ul",[e("li",[e("a",{attrs:{href:"https://github.com/pytorch/pytorch/issues/30664",target:"_blank",rel:"nofollow noopener noreferrer"}},[t._v("issues"),e("OutboundLink")],1)])])])]),t._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("Something went wrong Expecting value: line 1 column 1\n")])]),t._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[t._v("1")]),e("br")])]),e("p",[t._v("web_demo.py起来输入文字发送报错")]),t._v(" "),e("ul",[e("li",[e("p",[t._v("原因")]),t._v(" "),e("p",[t._v("开了vpn或代理导致，关闭即可")])]),t._v(" "),e("li",[e("p",[t._v("参考")]),t._v(" "),e("ul",[e("li",[e("a",{attrs:{href:"https://github.com/THUDM/ChatGLM-6B/issues/702",target:"_blank",rel:"nofollow noopener noreferrer"}},[t._v("issues"),e("OutboundLink")],1)])])])]),t._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("RuntimeError: Internal: D:\\a\\sentencepiece\\sentencepiece\\src\\sentencepiece_processor.cc(1102) [model_proto->ParseFromArray(serialized.data(), serialized.size())]\n")])]),t._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[t._v("1")]),e("br")])]),e("ul",[e("li",[e("p",[t._v("原因")]),t._v(" "),e("p",[t._v("通常是AutoTokenizer.from_pretrained和AutoModel.from_pretrained加载的模型不一致")])])]),t._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("OSError: You seem to have cloned a repository without having git-lfs installed. Please install git-lfs and run `git lfs install` followed by `git lfs pull` in the folder you cloned\n")])]),t._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[t._v("1")]),e("br")])]),e("ul",[e("li",[e("p",[t._v("原因")]),t._v(" "),e("p",[t._v("安装git-lfs")])])]),t._v(" "),e("h2",{attrs:{id:"参考"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#参考"}},[t._v("#")]),t._v(" 参考")]),t._v(" "),e("ul",[e("li",[t._v("[1]  "),e("a",{attrs:{href:"https://chatglm.cn/blog",target:"_blank",rel:"nofollow noopener noreferrer"}},[t._v("chatGLM"),e("OutboundLink")],1)]),t._v(" "),e("li",[t._v("[2]  "),e("a",{attrs:{href:"https://github.com/THUDM/ChatGLM-6B",target:"_blank",rel:"nofollow noopener noreferrer"}},[t._v("chatGLM-6B-github"),e("OutboundLink")],1)]),t._v(" "),e("li",[t._v("[3]  "),e("a",{attrs:{href:"https://cloud.tsinghua.edu.cn/d/674208019e314311ab5c/?p=%2Fchatglm-6b-int4&mode=list",target:"_blank",rel:"nofollow noopener noreferrer"}},[t._v("int4量化参数-配合模型实现"),e("OutboundLink")],1)]),t._v(" "),e("li",[t._v("[4]  "),e("a",{attrs:{href:"https://huggingface.co/THUDM/chatglm-6b-int4/tree/main",target:"_blank",rel:"nofollow noopener noreferrer"}},[t._v("int4量化-模型实现"),e("OutboundLink")],1)]),t._v(" "),e("li",[t._v("[5]  "),e("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/94220564",target:"_blank",rel:"nofollow noopener noreferrer"}},[t._v("win10安装CUDA cuDNN安装"),e("OutboundLink")],1)]),t._v(" "),e("li",[t._v("[6]  "),e("a",{attrs:{href:"https://blog.csdn.net/q116975174/article/details/130034839",target:"_blank",rel:"nofollow noopener noreferrer"}},[t._v("参考部署1"),e("OutboundLink")],1)]),t._v(" "),e("li",[t._v("[7]  "),e("a",{attrs:{href:"https://blog.csdn.net/qq_21201267/article/details/130176830",target:"_blank",rel:"nofollow noopener noreferrer"}},[t._v("参考部署2"),e("OutboundLink")],1)]),t._v(" "),e("li",[t._v("[8]  "),e("a",{attrs:{href:"https://k8sgpt.ai/",target:"_blank",rel:"nofollow noopener noreferrer"}},[t._v("友链k8sGPT"),e("OutboundLink")],1)]),t._v(" "),e("li",[t._v("[9]  "),e("a",{attrs:{href:"https://pytorch.org/get-started/locally/",target:"_blank",rel:"nofollow noopener noreferrer"}},[t._v("pytorch安装"),e("OutboundLink")],1)])])])}),[],!1,null,null,null);e.default=s.exports}}]);